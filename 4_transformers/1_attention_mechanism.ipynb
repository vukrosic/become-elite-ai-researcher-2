{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Attention Mechanism\n",
        "\n",
        "Attention allows models to focus on relevant parts of the input when making predictions.\n",
        "It's the core innovation behind transformers!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Attention?\n",
        "\n",
        "Attention computes how much each element in a sequence should \"attend to\" (focus on) each other element.\n",
        "\n",
        "The key idea: **Not all relationships are equally important!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple example: computing attention scores\n",
        "# Imagine we have 3 words: [\"cat\", \"sat\", \"mat\"]\n",
        "# We want to know how much \"cat\" attends to \"sat\" and \"mat\"\n",
        "\n",
        "# Represent words as vectors (embeddings)\n",
        "word_embeddings = torch.tensor([\n",
        "    [1.0, 2.0],  # \"cat\"\n",
        "    [0.5, 1.5],  # \"sat\"\n",
        "    [1.5, 0.5],  # \"mat\"\n",
        "])\n",
        "\n",
        "# Query: what we're looking for (the word \"cat\")\n",
        "query = word_embeddings[0]  # \"cat\"\n",
        "\n",
        "# Keys: what we're comparing against (all words)\n",
        "keys = word_embeddings\n",
        "\n",
        "# Compute attention scores: dot product between query and each key\n",
        "attention_scores = torch.matmul(query, keys.T)\n",
        "\n",
        "print(\"Word embeddings:\")\n",
        "print(word_embeddings)\n",
        "print(f\"\\nQuery (cat): {query}\")\n",
        "print(f\"\\nAttention scores:\")\n",
        "print(f\"  cat -> cat: {attention_scores[0]:.2f}\")\n",
        "print(f\"  cat -> sat: {attention_scores[1]:.2f}\")\n",
        "print(f\"  cat -> mat: {attention_scores[2]:.2f}\")\n",
        "\n",
        "# Normalize with softmax to get attention weights\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "print(f\"\\nAttention weights (after softmax):\")\n",
        "print(f\"  cat -> cat: {attention_weights[0]:.3f}\")\n",
        "print(f\"  cat -> sat: {attention_weights[1]:.3f}\")\n",
        "print(f\"  cat -> mat: {attention_weights[2]:.3f}\")\n",
        "print(f\"\\nSum: {attention_weights.sum():.3f} (should be 1.0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Formula\n",
        "\n",
        "Attention(Q, K, V) = softmax(QK^T / √d_k) × V\n",
        "\n",
        "Where:\n",
        "- **Q** (Query): What we're looking for\n",
        "- **K** (Key): What we're comparing against\n",
        "- **V** (Value): The actual information we extract\n",
        "- **d_k**: Dimension of keys (for scaling)\n",
        "§"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing scaled dot-product attention\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Compute scaled dot-product attention\n",
        "    \n",
        "    Args:\n",
        "        Q: Query tensor [batch_size, seq_len, d_k]\n",
        "        K: Key tensor [batch_size, seq_len, d_k]\n",
        "        V: Value tensor [batch_size, seq_len, d_v]\n",
        "    \n",
        "    Returns:\n",
        "        Output tensor and attention weights\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "    \n",
        "    # Compute attention scores: QK^T\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    \n",
        "    # Scale by sqrt(d_k)\n",
        "    scores = scores / np.sqrt(d_k)\n",
        "    \n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Apply weights to values\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Example usage\n",
        "batch_size, seq_len, d_k = 1, 4, 8\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, d_k)\n",
        "K = torch.randn(batch_size, seq_len, d_k)\n",
        "V = torch.randn(batch_size, seq_len, d_k)\n",
        "\n",
        "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(f\"Input shape - Q: {Q.shape}, K: {K.shape}, V: {V.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "print(f\"\\nAttention weights (first sequence):\")\n",
        "print(attn_weights[0])\n",
        "print(f\"\\nEach row sums to 1: {attn_weights[0].sum(dim=-1)}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
