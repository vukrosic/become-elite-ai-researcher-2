{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Self-Attention\n",
        "\n",
        "Self-attention is when a sequence attends to itself. Each position can attend to all positions, including itself!\n",
        "This is the building block of transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention: Q, K, V from Same Input\n",
        "\n",
        "In self-attention, Q, K, and V all come from the same input sequence.\n",
        "We use learned linear transformations to create Q, K, V from the input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create Q, K, V from input\n",
        "seq_len, d_model = 5, 8\n",
        "\n",
        "# Input sequence (e.g., word embeddings)\n",
        "x = torch.randn(seq_len, d_model)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "# Linear transformations to create Q, K, V\n",
        "W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "Q = W_q(x)  # Query\n",
        "K = W_k(x)  # Key\n",
        "V = W_v(x)  # Value\n",
        "\n",
        "print(f\"Q shape: {Q.shape}\")\n",
        "print(f\"K shape: {K.shape}\")\n",
        "print(f\"V shape: {V.shape}\")\n",
        "print(\"\\nIn self-attention, Q, K, V all come from the same input x!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention Layer\n",
        "\n",
        "Let's build a complete self-attention layer from scratch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention layer from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Linear layers to create Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, d_model]\n",
        "            attention_weights: [batch_size, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        \n",
        "        # Create Q, K, V\n",
        "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
        "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
        "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Compute attention scores: QK^T\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))  # [batch_size, seq_len, seq_len]\n",
        "        \n",
        "        # Scale by sqrt(d_model)\n",
        "        scores = scores / np.sqrt(d_model)\n",
        "        \n",
        "        # Softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)  # [batch_size, seq_len, seq_len]\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Test the self-attention layer\n",
        "d_model = 64\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "self_attn = SelfAttention(d_model)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "output, attn_weights = self_attn(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
        "print(f\"\\nAttention weights for first sequence, first position:\")\n",
        "print(attn_weights[0, 0])\n",
        "print(f\"Sum: {attn_weights[0, 0].sum():.3f} (should be 1.0)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
