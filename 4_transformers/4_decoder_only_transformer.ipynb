{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Decoder-Only Transformer\n",
        "\n",
        "Decoder-only transformers (like GPT) use masked self-attention and feed-forward networks.\n",
        "This architecture powers modern language models!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Components\n",
        "\n",
        "1. **Masked Multi-Head Attention** - Can only attend to previous positions\n",
        "2. **Feed-Forward Network** - Two linear layers with activation\n",
        "3. **Layer Normalization** - Normalizes activations\n",
        "4. **Residual Connections** - Adds input to output (x + f(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Masked attention: prevent attending to future positions\n",
        "seq_len = 5\n",
        "\n",
        "# Create causal mask (lower triangular)\n",
        "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "mask = mask.masked_fill(mask == 0, 0.0)\n",
        "\n",
        "print(\"Causal mask (0 = allowed, -inf = masked):\")\n",
        "print(mask)\n",
        "print(\"\\nEach position can only attend to itself and previous positions!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Masked Multi-Head Attention\n",
        "\n",
        "Same as multi-head attention, but with causal masking applied to attention scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    \"\"\"Masked multi-head attention for decoder\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "            mask: Optional causal mask [seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        \n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "        \n",
        "        # Reshape for multi-head\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "        \n",
        "        # Apply causal mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == float('-inf'), float('-inf'))\n",
        "        \n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
        "        attention_output = attention_output.view(batch_size, seq_len, d_model)\n",
        "        \n",
        "        output = self.W_o(attention_output)\n",
        "        return output\n",
        "\n",
        "# Test masked attention\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "# Create causal mask\n",
        "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))\n",
        "\n",
        "mha = MaskedMultiHeadAttention(d_model, num_heads)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "output = mha(x, mask=causal_mask)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"Masked multi-head attention with causal masking!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Feed-forward network\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Expand: d_model -> d_ff, then contract: d_ff -> d_model\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# Test feed-forward\n",
        "d_model = 64\n",
        "d_ff = 256  # Typically 4x d_model\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "ff = FeedForward(d_model, d_ff)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "output = ff(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Expanded to {d_ff} dimensions, then back to {d_model}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer Decoder Block\n",
        "\n",
        "Combines masked attention, feed-forward, layer norm, and residual connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    \"\"\"Single decoder block\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MaskedMultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual and norm\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # Feed-forward with residual and norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test decoder block\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "d_ff = 256\n",
        "seq_len = 10\n",
        "batch_size = 2\n",
        "\n",
        "causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))\n",
        "\n",
        "block = TransformerDecoderBlock(d_model, num_heads, d_ff)\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "output = block(x, mask=causal_mask)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(\"Decoder block with masked attention, feed-forward, and residuals!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Decoder-Only Transformer\n",
        "\n",
        "Stack multiple decoder blocks to create a full transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    \"\"\"Decoder-only transformer (GPT-style)\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Token and positional embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        \n",
        "        # Stack of decoder blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerDecoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Token indices [batch_size, seq_len]\n",
        "        Returns:\n",
        "            logits: [batch_size, seq_len, vocab_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = x.shape\n",
        "        \n",
        "        # Create causal mask\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        \n",
        "        # Token embeddings\n",
        "        token_emb = self.token_embedding(x)  # [batch_size, seq_len, d_model]\n",
        "        \n",
        "        # Positional embeddings\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.position_embedding(positions)  # [1, seq_len, d_model]\n",
        "        \n",
        "        # Combine embeddings\n",
        "        x = self.dropout(token_emb + pos_emb)\n",
        "        \n",
        "        # Pass through decoder blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask=mask)\n",
        "        \n",
        "        # Final layer norm and projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# Test complete transformer\n",
        "vocab_size = 1000\n",
        "d_model = 128\n",
        "num_heads = 8\n",
        "num_layers = 4\n",
        "d_ff = 512\n",
        "max_seq_len = 256\n",
        "\n",
        "model = DecoderOnlyTransformer(\n",
        "    vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len\n",
        ")\n",
        "\n",
        "# Example input: token indices\n",
        "batch_size = 2\n",
        "seq_len = 20\n",
        "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "logits = model(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output logits shape: {logits.shape}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"\\nDecoder-only transformer complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
