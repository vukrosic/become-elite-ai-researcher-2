# ðŸŽ¯ The Path to Becoming an AI Researcher

### Phase 1: Mathematical Foundations ðŸ§®
**Location:** `1_math/`

- Math Functions & Derivatives
- Vectors & Gradients
- Matrices & Linear Algebra
- Probability & Statistics

**Why:** Derivatives enable backpropagation. Matrices are how layers transform data. This foundation is essential.

### Phase 2: PyTorch Fundamentals ðŸ”¥
**Location:** `2_pytorch/`

- Tensor Creation & Manipulation
- Matrix Operations
- Tensor Reshaping & Indexing
- Advanced Tensor Operations (concatenation, stacking)

**Why:** PyTorch is the standard research framework. Mastery here lets you implement any architecture.

### Phase 3: Neural Networks from Scratch ðŸ§ 
**Location:** `3_neural_networks/`

- Single Neuron Implementation
- Building Layers
- Backpropagation
- Normalization & Activation Functions
- Optimizers

**Why:** Building from scratch develops deep understanding of how networks actually work.

### Phase 4: Transformers ðŸ”„
**Location:** `4_transformers/`

- Attention Mechanism
- Self-Attention
- Multi-Head Attention
- Decoder-Only Transformer

**Why:** Transformers power modern language models. Understanding attention and decoder architecture enables you to work with GPT, BERT, and other state-of-the-art models.

## ðŸŽ“ How to Progress

1. **Follow sequential order** - Complete each phase before moving to the next
2. **Practice actively** - Run, modify, and experiment with code
3. **Focus on understanding** - Know *why* things work, not just *how*
4. **Don't rush** - Gaps in fundamentals will compound later

## ðŸš€ Outcomes

After completing this course, you will:
- Understand neural network mathematics deeply
- Manipulate tensors fluently in PyTorch
- Implement networks from scratch
- Read and implement research papers
- Debug training issues effectively

## ðŸ’¡ Approach

This course uses a bottom-up approach: fundamentals first, then building upward. This builds true understanding and enables independent research.

**Start with:** `1_math/1_math_functions_examples.ipynb`
