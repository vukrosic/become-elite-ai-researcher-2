{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 11. Learning Rate Schedulers\n",
                "\n",
                "Finding the right Learning Rate (LR) is hard.\n",
                "- **Too High**: Diverges.\n",
                "- **Too Low**: Takes forever.\n",
                "\n",
                "**Solution**: Start high, then lower it over time.\n",
                "This is what **LR Schedulers** do."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup\n",
                "\n",
                "We need an optimizer to attach the scheduler to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = nn.Linear(1, 1)\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. StepLR\n",
                "\n",
                "Decays the LR by a factor `gamma` every `step_size` epochs.\n",
                "Example: Drop LR by half every 10 epochs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
                "\n",
                "lrs = []\n",
                "for epoch in range(50):\n",
                "    optimizer.step() # Normally you'd do this after backward()\n",
                "    lrs.append(optimizer.param_groups[0]['lr'])\n",
                "    scheduler.step() # Update LR\n",
                "\n",
                "plt.plot(lrs)\n",
                "plt.title(\"StepLR\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Learning Rate\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. CosineAnnealingLR\n",
                "\n",
                "Smoothly decreases LR following a cosine curve. Very popular!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reset Optimizer\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
                "\n",
                "lrs = []\n",
                "for epoch in range(50):\n",
                "    optimizer.step()\n",
                "    lrs.append(optimizer.param_groups[0]['lr'])\n",
                "    scheduler.step()\n",
                "\n",
                "plt.plot(lrs)\n",
                "plt.title(\"CosineAnnealingLR\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"Learning Rate\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Where to put it in the Training Loop?\n",
                "\n",
                "```python\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
                "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    train(...)\n",
                "    validate(...)\n",
                "    \n",
                "    # Step the scheduler at the END of the epoch\n",
                "    scheduler.step()\n",
                "```\n",
                "\n",
                "**Exception**: `ReduceLROnPlateau` steps after validation, using the validation loss:\n",
                "```python\n",
                "scheduler.step(val_loss)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}