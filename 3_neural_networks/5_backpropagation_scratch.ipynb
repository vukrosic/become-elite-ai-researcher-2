{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Backpropagation From Scratch\n",
    "\n",
    "Backpropagation is the engine of neural networks! It's how they learn.\n",
    "It calculates gradients: how much each parameter contributed to the error.\n",
    "Let's build it from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Chain Rule & Computational Graph\n",
    "\n",
    "Neural networks are just big composite functions.\n",
    "To find the derivative of a composite function, we use the **Chain Rule**.\n",
    "\n",
    "If $y = f(u)$ and $u = g(x)$, then:\n",
    "$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$\n",
    "\n",
    "Let's trace a simple computation: $L = (wx + b - y_{true})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Manual Calculation Step-by-Step\n",
    "\n",
    "Let's calculate gradients for a single linear neuron manually.\n",
    "Forward pass:\n",
    "1. $z = w \\cdot x + b$\n",
    "2. $\\hat{y} = z$ (linear activation for simplicity)\n",
    "3. $L = (\\hat{y} - y_{true})^2$\n",
    "\n",
    "Backward pass (Chain Rule):\n",
    "We want $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$.\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y_{true})$\n",
    "2. $\\frac{\\partial \\hat{y}}{\\partial z} = 1$\n",
    "3. $\\frac{\\partial z}{\\partial w} = x$\n",
    "4. $\\frac{\\partial z}{\\partial b} = 1$\n",
    "\n",
    "Putting it together:\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = 2(\\hat{y} - y_{true}) \\cdot 1 \\cdot x$\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = 2(\\hat{y} - y_{true}) \\cdot 1 \\cdot 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Backpropagation Example\n",
    "\n",
    "# 1. Initialize parameters\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# 2. Input and Target\n",
    "x = torch.tensor(3.0)\n",
    "y_true = torch.tensor(10.0)\n",
    "\n",
    "# 3. Forward Pass\n",
    "z = w * x + b\n",
    "y_pred = z\n",
    "loss = (y_pred - y_true)**2\n",
    "\n",
    "print(f\"Forward Pass:\")\n",
    "print(f\"x={x}, w={w}, b={b}\")\n",
    "print(f\"y_pred = {w}*{x} + {b} = {y_pred}\")\n",
    "print(f\"loss = ({y_pred} - {y_true})^2 = {loss}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 4. Backward Pass (Manual)\n",
    "dL_dy_pred = 2 * (y_pred - y_true)\n",
    "dy_pred_dz = 1.0\n",
    "dz_dw = x\n",
    "dz_db = 1.0\n",
    "\n",
    "grad_w_manual = dL_dy_pred * dy_pred_dz * dz_dw\n",
    "grad_b_manual = dL_dy_pred * dy_pred_dz * dz_db\n",
    "\n",
    "print(f\"Manual Gradients:\")\n",
    "print(f\"dL/dw = {grad_w_manual}\")\n",
    "print(f\"dL/db = {grad_b_manual}\")\n",
    "\n",
    "# 5. Verify with PyTorch Autograd\n",
    "loss.backward()\n",
    "print(\"-\" * 30)\n",
    "print(f\"PyTorch Autograd Gradients:\")\n",
    "print(f\"w.grad = {w.grad}\")\n",
    "print(f\"b.grad = {b.grad}\")\n",
    "\n",
    "assert grad_w_manual == w.grad\n",
    "assert grad_b_manual == b.grad\n",
    "print(\"\\nMatch! We successfully implemented backprop manually!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a 'Linear' Layer with Backprop\n",
    "\n",
    "Now let's encapsulate this logic into a class.\n",
    "We need to store `inputs` during the forward pass to use them in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # Initialize weights and bias\n",
    "        self.W = torch.randn(out_features, in_features) * 0.1\n",
    "        self.b = torch.zeros(out_features)\n",
    "        \n",
    "        # Gradients storage\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.input_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: y = Wx + b\n",
    "        x shape: (batch_size, in_features)\n",
    "        output shape: (batch_size, out_features)\n",
    "        \"\"\"\n",
    "        self.input_cache = x  # Store input for backward pass\n",
    "        return x @ self.W.T + self.b\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        grad_output: dL/dy (gradient of loss w.r.t output of this layer)\n",
    "        \"\"\"\n",
    "        x = self.input_cache\n",
    "        \n",
    "        # 1. Calculate gradients for parameters\n",
    "        # dL/dW = (dL/dy)^T * x\n",
    "        self.grad_W = grad_output.T @ x\n",
    "        \n",
    "        # dL/db = sum(dL/dy) across batch\n",
    "        self.grad_b = grad_output.sum(dim=0)\n",
    "        \n",
    "        # 2. Calculate gradient for input (to pass to previous layer)\n",
    "        # dL/dx = dL/dy * W\n",
    "        grad_input = grad_output @ self.W\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building MSE Loss with Backprop\n",
    "\n",
    "We also need a loss function that can compute its own gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.pred_cache = None\n",
    "        self.target_cache = None\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        self.pred_cache = pred\n",
    "        self.target_cache = target\n",
    "        return ((pred - target) ** 2).mean()\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Computes dL/d_pred\n",
    "        L = mean((pred - target)^2)\n",
    "        dL/d_pred = 2 * (pred - target) / batch_size\n",
    "        \"\"\"\n",
    "        n = self.pred_cache.shape[0]\n",
    "        return 2 * (self.pred_cache - self.target_cache) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop from Scratch\n",
    "\n",
    "Let's train our custom Linear layer to learn a simple function: $y = 3x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "X = torch.rand(100, 1) * 5  # Inputs 0-5\n",
    "Y = 3 * X + 2 + torch.randn(100, 1) * 0.1  # y = 3x + 2 + noise\n",
    "\n",
    "# Initialize model and loss\n",
    "model = Linear(in_features=1, out_features=1)\n",
    "criterion = MSELoss()\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "print(f\"Initial weights: W={model.W.item():.4f}, b={model.b.item():.4f}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1. Forward Pass\n",
    "    y_pred = model.forward(X)\n",
    "    loss = criterion.forward(y_pred, Y)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 2. Backward Pass\n",
    "    grad_loss = criterion.backward()\n",
    "    model.backward(grad_loss)\n",
    "    \n",
    "    # 3. Update Parameters (SGD)\n",
    "    model.W -= learning_rate * model.grad_W\n",
    "    model.b -= learning_rate * model.grad_b\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal weights: W={model.W.item():.4f}, b={model.b.item():.4f}\")\n",
    "print(f\"Target weights: W=3.0000, b=2.0000\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "We just built backpropagation from scratch!\n",
    "1. We understood the **Chain Rule**.\n",
    "2. We calculated gradients **manually**.\n",
    "3. We implemented a **Linear Layer** that computes its own gradients.\n",
    "4. We trained it to learn a function!\n",
    "\n",
    "This is exactly what PyTorch does under the hood with `autograd`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
