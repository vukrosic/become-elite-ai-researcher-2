{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8. Weight Initialization\n",
                "\n",
                "How you start matters.\n",
                "If weights are too small, signals vanish.\n",
                "If weights are too large, signals explode.\n",
                "Let's see how to initialize them \"just right\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Problem: Vanishing & Exploding Gradients\n",
                "\n",
                "Let's simulate a deep network by multiplying a vector by many random matrices."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_deep_network(init_std, activation_fn=None):\n",
                "    x = torch.randn(512)\n",
                "    activations = [x]\n",
                "    \n",
                "    for i in range(10): # 10 layers\n",
                "        W = torch.randn(512, 512) * init_std\n",
                "        x = W @ x\n",
                "        if activation_fn:\n",
                "            x = activation_fn(x)\n",
                "        activations.append(x)\n",
                "        \n",
                "    return activations\n",
                "\n",
                "def plot_activations(activations, title):\n",
                "    plt.figure(figsize=(10, 4))\n",
                "    for i, act in enumerate(activations):\n",
                "        plt.subplot(1, len(activations), i+1)\n",
                "        plt.hist(act.numpy(), bins=30, range=(-3, 3))\n",
                "        plt.axis('off')\n",
                "    plt.suptitle(title)\n",
                "    plt.show()\n",
                "\n",
                "# 1. Small Weights (std=0.01)\n",
                "acts = simulate_deep_network(0.01)\n",
                "plot_activations(acts, \"Small Weights: Vanishing Signal (Everything becomes 0)\")\n",
                "\n",
                "# 2. Large Weights (std=1.0)\n",
                "acts = simulate_deep_network(1.0)\n",
                "plot_activations(acts, \"Large Weights: Exploding Signal (Values get huge)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Xavier (Glorot) Initialization\n",
                "\n",
                "Designed for **Sigmoid** and **Tanh** activations.\n",
                "Goal: Keep the variance of activations constant across layers.\n",
                "\n",
                "$W \\sim N(0, \\frac{2}{n_{in} + n_{out}})$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Xavier Initialization\n",
                "n_in = 512\n",
                "n_out = 512\n",
                "xavier_std = np.sqrt(2 / (n_in + n_out))\n",
                "\n",
                "acts = simulate_deep_network(xavier_std, torch.tanh)\n",
                "plot_activations(acts, \"Xavier Init with Tanh: Stable!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Kaiming (He) Initialization\n",
                "\n",
                "Designed for **ReLU** activations.\n",
                "ReLU kills half the neurons (sets them to 0), so we need to double the variance to compensate.\n",
                "\n",
                "$W \\sim N(0, \\frac{2}{n_{in}})$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Kaiming Initialization\n",
                "kaiming_std = np.sqrt(2 / n_in)\n",
                "\n",
                "acts = simulate_deep_network(kaiming_std, torch.relu)\n",
                "plot_activations(acts, \"Kaiming Init with ReLU: Stable!\")\n",
                "\n",
                "# What if we used Xavier with ReLU?\n",
                "acts = simulate_deep_network(xavier_std, torch.relu)\n",
                "plot_activations(acts, \"Xavier Init with ReLU: Signal Vanishes!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. How to use in PyTorch\n",
                "\n",
                "PyTorch layers are initialized reasonably by default, but you can override them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "layer = nn.Linear(512, 512)\n",
                "\n",
                "# Apply Kaiming Normal\n",
                "nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
                "\n",
                "# Apply Xavier Uniform\n",
                "nn.init.xavier_uniform_(layer.weight)\n",
                "\n",
                "print(\"Weights initialized!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}