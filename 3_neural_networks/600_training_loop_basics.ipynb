{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 6. The Training Loop\n",
                "\n",
                "You have the components: Model, Loss, Optimizer.\n",
                "Now let's put them together in a **Training Loop**.\n",
                "This is the heartbeat of deep learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The 4 Steps of Training\n",
                "\n",
                "For every batch of data:\n",
                "1. **Forward Pass**: Compute predictions.\n",
                "2. **Compute Loss**: How bad were the predictions?\n",
                "3. **Backward Pass**: Calculate gradients (`loss.backward()`).\n",
                "4. **Optimizer Step**: Update weights (`optimizer.step()`).\n",
                "\n",
                "**Crucial Step**: Don't forget `optimizer.zero_grad()` before backward!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Data (Linear Regression: y = 2x + 1)\n",
                "X = torch.linspace(0, 10, 100).view(-1, 1)\n",
                "y = 2 * X + 1 + torch.randn(100, 1) * 0.5 # Add noise\n",
                "\n",
                "# 2. Model\n",
                "model = nn.Linear(1, 1)\n",
                "\n",
                "# 3. Loss and Optimizer\n",
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
                "\n",
                "# Training Loop\n",
                "epochs = 100\n",
                "losses = []\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    # Step 0: Zero gradients (otherwise they accumulate!)\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    # Step 1: Forward Pass\n",
                "    y_pred = model(X)\n",
                "    \n",
                "    # Step 2: Compute Loss\n",
                "    loss = criterion(y_pred, y)\n",
                "    losses.append(loss.item())\n",
                "    \n",
                "    # Step 3: Backward Pass\n",
                "    loss.backward()\n",
                "    \n",
                "    # Step 4: Update Weights\n",
                "    optimizer.step()\n",
                "    \n",
                "    if epoch % 10 == 0:\n",
                "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
                "\n",
                "print(\"Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Visualizing Results\n",
                "\n",
                "Let's see if the model learned the line $y = 2x + 1$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Loss\n",
                "plt.figure(figsize=(12, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(losses)\n",
                "plt.title(\"Training Loss\")\n",
                "plt.xlabel(\"Epoch\")\n",
                "plt.ylabel(\"MSE\")\n",
                "\n",
                "# Plot Predictions\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.scatter(X, y, alpha=0.5, label=\"Data\")\n",
                "plt.plot(X, model(X).detach(), 'r', label=\"Prediction\")\n",
                "plt.title(\"Model Fit\")\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Learned Weights: w={model.weight.item():.2f}, b={model.bias.item():.2f}\")\n",
                "print(\"True Weights:    w=2.00, b=1.00\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Common Pitfalls\n",
                "\n",
                "1. **Forgetting `zero_grad()`**: Gradients will add up, leading to huge updates and exploding loss.\n",
                "2. **Wrong Learning Rate**: \n",
                "    - Too high: Loss explodes (NaN).\n",
                "    - Too low: Loss doesn't decrease.\n",
                "3. **Not detaching for plotting**: `model(X)` has gradients. Use `.detach()` or `with torch.no_grad():` when plotting or evaluating."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}