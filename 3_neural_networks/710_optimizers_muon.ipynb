{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2a1b22e2",
            "metadata": {},
            "source": [
                "# ðŸš€ Deep Dive: The Muon Optimizer\n",
                "\n",
                "Welcome to this step-by-step tutorial on **Muon** (MomentUm Orthogonalized by Newton-schulz). \n",
                "\n",
                "In this notebook, we will:\n",
                "1.  **Understand the Problem**: Why do we need another optimizer? What is \"Gradient Orthogonalization\"?\n",
                "2.  **Explore the Math**: How Newton-Schulz iteration works to normalize matrices.\n",
                "3.  **Implement Muon**: Step-by-step build of the optimizer class.\n",
                "4.  **Compare Performance**: Pit Muon against SGD and Adam on a challenging optimization landscape.\n",
                "5.  **Visualize Internals**: See how Muon affects the singular values of weight matrices."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "691d1532",
            "metadata": {},
            "source": [
                "## 1. The Problem: Scaling and Curvature\n",
                "\n",
                "### The Landscape\n",
                "Imagine training a neural network as walking down a mountain. \n",
                "-   **SGD** takes steps based on the slope immediately under your feet. If the slope is steep in one direction and flat in another (a ravine), SGD oscillates or moves slowly.\n",
                "-   **Adam** scales the step size for each parameter individually (diagonal scaling). This helps with axis-aligned ravines but ignores correlations between parameters.\n",
                "\n",
                "### The Muon Approach\n",
                "Muon goes a step further. Instead of just scaling individual parameters, it considers the **entire geometry** of the weight matrix update. It tries to **orthogonalize** the update steps.\n",
                "\n",
                "**Why?** \n",
                "In deep learning, weight matrices often develop a few very large \"singular values\" (dominant directions) that overshadow everything else. Gradients in these directions are huge, while gradients in other useful directions are tiny. \n",
                "\n",
                "Muon forces the update matrix to be \"orthogonal\" (or close to it). This effectively flattens the curvature in all directions, allowing the optimizer to make progress everywhere at once. It's like turning a narrow ravine into a smooth bowl."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac068738",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "97aa5a7f",
            "metadata": {},
            "source": [
                "## 2. The Core: Newton-Schulz Iteration\n",
                "\n",
                "To orthogonalize a matrix $G$, we want to map it to $U V^T$ where $G = U \\Sigma V^T$ is the SVD. This is equivalent to $G (G^T G)^{-1/2}$.\n",
                "\n",
                "Computing SVD is very slow on GPUs. **Newton-Schulz iteration** is a fast, iterative method to approximate this without calculating eigenvalues explicitly. It only uses matrix multiplications!\n",
                "\n",
                "Let's look at the provided implementation:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9517a0a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.compile\n",
                "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
                "    \"\"\"\n",
                "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.\n",
                "    We want to find X such that X @ X.T is approximately Identity.\n",
                "    \"\"\"\n",
                "    assert G.ndim >= 2\n",
                "    \n",
                "    # Constants for the quintic (5th order) iteration\n",
                "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
                "    X = G.half() # Run in FP16 for speed/memory, usually sufficient for updates\n",
                "\n",
                "    # Ensure we are working with a 'short and fat' or square matrix for stability\n",
                "    # If it's 'tall and skinny', we transpose it.\n",
                "    if G.size(-2) > G.size(-1):\n",
                "        X = X.mT\n",
                "\n",
                "    # Preconditioning: Scale X so its norm is close to 1. \n",
                "    # This is crucial for Newton-Schulz convergence.\n",
                "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
                "\n",
                "    # The Iteration Loop\n",
                "    for _ in range(steps):\n",
                "        A = X @ X.mT\n",
                "        # The update rule: X_new = aX + bAX + cA^2X\n",
                "        B = b * A + c * A @ A\n",
                "        X = a * X + B @ X\n",
                "\n",
                "    # Untranspose if we transposed earlier\n",
                "    if G.size(-2) > G.size(-1):\n",
                "        X = X.mT\n",
                "\n",
                "    return X"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2c45ca14",
            "metadata": {},
            "source": [
                "### ðŸ§ª Interactive Experiment: Visualizing Orthogonalization\n",
                "\n",
                "Let's create a random, ill-conditioned matrix (one direction is much stronger than others) and see what `zeropower_via_newtonschulz5` does to it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24774848",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Create a random matrix\n",
                "torch.manual_seed(0)\n",
                "G = torch.randn(100, 100).to(device)\n",
                "\n",
                "# 2. Make it ill-conditioned (multiply one dimension by 100)\n",
                "G[:, 0] *= 100\n",
                "\n",
                "# 3. Apply Newton-Schulz\n",
                "O = zeropower_via_newtonschulz5(G, steps=5)\n",
                "\n",
                "# 4. Check Orthogonality: O @ O.T should be close to Identity\n",
                "gram = O @ O.mT\n",
                "identity = torch.eye(100, device=device).half()\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.title(\"Original G @ G.T (Log Scale)\")\n",
                "plt.imshow((G @ G.mT).abs().log().cpu().float(), cmap='viridis')\n",
                "plt.colorbar()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.title(\"Orthogonalized O @ O.T\")\n",
                "plt.imshow(gram.cpu().float(), cmap='viridis')\n",
                "plt.colorbar()\n",
                "\n",
                "plt.show()\n",
                "\n",
                "print(f\"Max deviation from Identity: {(gram - identity).abs().max().item():.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef8125b8",
            "metadata": {},
            "source": [
                "**Observation**: The original matrix product has huge values (yellow spots). The orthogonalized version is a clean diagonal line (Identity matrix). This means `O` has successfully equalized the energy in all directions!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "470093ab",
            "metadata": {},
            "source": [
                "## 3. Implementing the Optimizer\n",
                "\n",
                "Now we wrap this logic into a PyTorch Optimizer.\n",
                "\n",
                "**Key Features**:\n",
                "1.  **Momentum**: We don't just orthogonalize the raw gradient `g`. We maintain a momentum buffer `buf` (moving average of gradients) and orthogonalize *that*.\n",
                "2.  **Nesterov**: Optionally applies Nesterov momentum correction.\n",
                "3.  **Scaling**: The update is scaled by `max(1, rows/cols)**0.5`. This is a heuristic to handle rectangular matrices correctly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f02446f3",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Muon(torch.optim.Optimizer):\n",
                "    \"\"\"Muon - MomentUm Orthogonalized by Newton-schulz\"\"\"\n",
                "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
                "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
                "        super().__init__(params, defaults)\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def step(self):\n",
                "        for group in self.param_groups:\n",
                "            for p in group[\"params\"]:\n",
                "                if p.grad is None:\n",
                "                    continue\n",
                "\n",
                "                g = p.grad\n",
                "                state = self.state[p]\n",
                "\n",
                "                # 1. Initialize Momentum Buffer\n",
                "                if \"momentum_buffer\" not in state:\n",
                "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
                "\n",
                "                # 2. Update Momentum (using lerp for stability)\n",
                "                # buf = buf * momentum + g * (1 - momentum)\n",
                "                buf = state[\"momentum_buffer\"]\n",
                "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
                "                \n",
                "                # 3. Nesterov Correction (optional)\n",
                "                # If Nesterov, we use the 'lookahead' gradient for the update\n",
                "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
                "                \n",
                "                # 4. Orthogonalize the Update\n",
                "                # This is the MAGIC step. Instead of just subtracting g, \n",
                "                # we subtract a \"whitened\" version of g.\n",
                "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
                "                \n",
                "                # 5. Apply Update\n",
                "                # We scale the learning rate based on matrix shape.\n",
                "                scale_factor = max(1, p.size(-2) / p.size(-1))**0.5\n",
                "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * scale_factor)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6697da77",
            "metadata": {},
            "source": [
                "## 4. The Showdown: Muon vs. Adam vs. SGD\n",
                "\n",
                "We will train a Deep Linear Network on a synthetic task. Deep linear networks are notoriously hard to train because gradients vanish or explode, and the curvature becomes very ill-conditioned.\n",
                "\n",
                "**Task**: Learn to map a 64-dim input to a 64-dim output via a 10-layer network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fbf4d660",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "N_LAYERS = 10\n",
                "DIM = 64\n",
                "BATCH_SIZE = 128\n",
                "STEPS = 500\n",
                "\n",
                "# Synthetic Data\n",
                "X_train = torch.randn(1000, DIM, device=device)\n",
                "Y_train = torch.randn(1000, DIM, device=device) # Random target\n",
                "\n",
                "def get_model():\n",
                "    layers = []\n",
                "    for _ in range(N_LAYERS):\n",
                "        # bias=False is important for this specific Muon implementation \n",
                "        # because it expects 2D tensors (matrices).\n",
                "        layers.append(nn.Linear(DIM, DIM, bias=False))\n",
                "    return nn.Sequential(*layers).to(device)\n",
                "\n",
                "def train_optimizer(optim_cls, name, lr, **kwargs):\n",
                "    torch.manual_seed(42)\n",
                "    model = get_model()\n",
                "    optimizer = optim_cls(model.parameters(), lr=lr, **kwargs)\n",
                "    \n",
                "    losses = []\n",
                "    \n",
                "    for step in range(STEPS):\n",
                "        # Batch sampling\n",
                "        indices = torch.randint(0, 1000, (BATCH_SIZE,))\n",
                "        x_batch = X_train[indices]\n",
                "        y_batch = Y_train[indices]\n",
                "        \n",
                "        # Forward\n",
                "        pred = model(x_batch)\n",
                "        loss = F.mse_loss(pred, y_batch)\n",
                "        \n",
                "        # Backward\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        losses.append(loss.item())\n",
                "        \n",
                "    return losses\n",
                "\n",
                "print(\"Training SGD...\")\n",
                "loss_sgd = train_optimizer(torch.optim.SGD, \"SGD\", lr=0.01, momentum=0.9)\n",
                "\n",
                "print(\"Training Adam...\")\n",
                "loss_adam = train_optimizer(torch.optim.Adam, \"Adam\", lr=0.001)\n",
                "\n",
                "print(\"Training Muon...\")\n",
                "loss_muon = train_optimizer(Muon, \"Muon\", lr=0.02)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b0e8f959",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plotting Results\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(loss_sgd, label='SGD (0.01)', alpha=0.7)\n",
                "plt.plot(loss_adam, label='Adam (0.001)', alpha=0.7)\n",
                "plt.plot(loss_muon, label='Muon (0.02)', linewidth=2, color='red')\n",
                "\n",
                "plt.yscale('log')\n",
                "plt.title(f\"Optimization Speed on {N_LAYERS}-Layer Linear Network\")\n",
                "plt.xlabel(\"Steps\")\n",
                "plt.ylabel(\"MSE Loss (Log Scale)\")\n",
                "plt.legend()\n",
                "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "58ca3cb8",
            "metadata": {},
            "source": [
                "## 5. Analysis: Why did Muon win?\n",
                "\n",
                "In deep linear networks, the singular values of the weight matrices tend to spread out. Some become huge, some tiny. \n",
                "-   **SGD** gets stuck bouncing between the huge directions.\n",
                "-   **Muon** forces the update to be orthogonal. This means it pushes equally hard in *all* directions, effectively ignoring the fact that some directions are \"steep\" and others are \"flat\". It traverses the landscape much more efficiently.\n",
                "\n",
                "### Summary\n",
                "1.  **Muon** combines Momentum with Newton-Schulz orthogonalization.\n",
                "2.  It is particularly effective for **large-scale training** (like Transformers) where spectral scaling issues are prominent.\n",
                "3.  It adds a computational cost (the matrix multiplications in Newton-Schulz), but often converges in far fewer steps, saving total time."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
