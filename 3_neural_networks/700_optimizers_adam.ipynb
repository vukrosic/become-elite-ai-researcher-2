{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Adam Optimizer From Scratch\n",
                "\n",
                "Adam (Adaptive Moment Estimation) is one of the most popular optimizers in deep learning.\n",
                "It combines the best of **Momentum** and **RMSProp**.\n",
                "Let's build it from scratch and see why it's so good!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Concepts: SGD vs. Adam\n",
                "\n",
                "### SGD (Stochastic Gradient Descent)\n",
                "Updates parameters by subtracting the gradient multiplied by a learning rate.\n",
                "$w_{t+1} = w_t - \\eta \\cdot \\nabla L(w_t)$\n",
                "\n",
                "**Problem**: It can get stuck in local minima or oscillate in ravines.\n",
                "\n",
                "### Adam\n",
                "Adam keeps track of two things for each parameter:\n",
                "1. **Momentum ($m$)**: The moving average of gradients (like a heavy ball rolling down).\n",
                "2. **Variance ($v$)**: The moving average of squared gradients (scales learning rate based on how much the gradient changes).\n",
                "\n",
                "Update rule:\n",
                "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$ (First Moment)\n",
                "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$ (Second Moment)\n",
                "\n",
                "Bias correction:\n",
                "$\\hat{m}_t = m_t / (1 - \\beta_1^t)$\n",
                "$\\hat{v}_t = v_t / (1 - \\beta_2^t)$\n",
                "\n",
                "Parameter update:\n",
                "$w_{t+1} = w_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Implementing Optimizers\n",
                "\n",
                "Let's define a base class and implement SGD and Adam."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Optimizer:\n",
                "    def __init__(self, params, lr):\n",
                "        self.params = list(params)\n",
                "        self.lr = lr\n",
                "        \n",
                "    def step(self):\n",
                "        raise NotImplementedError\n",
                "        \n",
                "    def zero_grad(self):\n",
                "        for p in self.params:\n",
                "            if p.grad is not None:\n",
                "                p.grad.zero_()\n",
                "\n",
                "class SGD(Optimizer):\n",
                "    def step(self):\n",
                "        with torch.no_grad():\n",
                "            for p in self.params:\n",
                "                if p.grad is None: continue\n",
                "                p -= self.lr * p.grad\n",
                "\n",
                "class Adam(Optimizer):\n",
                "    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8):\n",
                "        super().__init__(params, lr)\n",
                "        self.betas = betas\n",
                "        self.eps = eps\n",
                "        self.m = [torch.zeros_like(p) for p in self.params]\n",
                "        self.v = [torch.zeros_like(p) for p in self.params]\n",
                "        self.t = 0\n",
                "        \n",
                "    def step(self):\n",
                "        self.t += 1\n",
                "        beta1, beta2 = self.betas\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            for i, p in enumerate(self.params):\n",
                "                if p.grad is None: continue\n",
                "                grad = p.grad\n",
                "                \n",
                "                # Update biased first moment estimate\n",
                "                self.m[i] = beta1 * self.m[i] + (1 - beta1) * grad\n",
                "                \n",
                "                # Update biased second raw moment estimate\n",
                "                self.v[i] = beta2 * self.v[i] + (1 - beta2) * (grad ** 2)\n",
                "                \n",
                "                # Compute bias-corrected first moment estimate\n",
                "                m_hat = self.m[i] / (1 - beta1 ** self.t)\n",
                "                \n",
                "                # Compute bias-corrected second raw moment estimate\n",
                "                v_hat = self.v[i] / (1 - beta2 ** self.t)\n",
                "                \n",
                "                # Update parameters\n",
                "                p -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Test: Rosenbrock Function (The Banana Valley)\n",
                "\n",
                "We'll test on a tricky function where SGD struggles: $f(x, y) = (1-x)^2 + 100(y-x^2)^2$.\n",
                "It has a global minimum at $(1, 1)$ inside a long, narrow, parabolic valley."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rosenbrock(x, y):\n",
                "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
                "\n",
                "def train_optimizer(optimizer_class, lr, steps=2000):\n",
                "    # Start at (-1.5, -1) - far from (1, 1)\n",
                "    x = torch.tensor([-1.5], requires_grad=True)\n",
                "    y = torch.tensor([-1.0], requires_grad=True)\n",
                "    \n",
                "    optimizer = optimizer_class([x, y], lr=lr)\n",
                "    path = []\n",
                "    \n",
                "    for _ in range(steps):\n",
                "        path.append((x.item(), y.item()))\n",
                "        \n",
                "        loss = rosenbrock(x, y)\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "    return np.array(path)\n",
                "\n",
                "# Run experiments\n",
                "print(\"Training SGD...\")\n",
                "path_sgd = train_optimizer(SGD, lr=0.001)\n",
                "\n",
                "print(\"Training Adam...\")\n",
                "path_adam = train_optimizer(Adam, lr=0.1) # Adam can handle larger LR\n",
                "\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualization: The Race\n",
                "\n",
                "Let's see who gets to the minimum (1, 1) faster!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a grid for contour plot\n",
                "x_grid = np.linspace(-2, 2, 100)\n",
                "y_grid = np.linspace(-1, 3, 100)\n",
                "X, Y = np.meshgrid(x_grid, y_grid)\n",
                "Z = (1 - X)**2 + 100 * (Y - X**2)**2\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='jet', alpha=0.5)\n",
                "plt.plot(1, 1, 'r*', markersize=15, label='Global Minimum (1, 1)')\n",
                "\n",
                "# Plot paths\n",
                "plt.plot(path_sgd[:, 0], path_sgd[:, 1], 'b-', label='SGD', linewidth=2)\n",
                "plt.plot(path_adam[:, 0], path_adam[:, 1], 'g-', label='Adam', linewidth=2)\n",
                "\n",
                "plt.legend()\n",
                "plt.title(\"SGD vs Adam on Rosenbrock Function\")\n",
                "plt.xlabel(\"x\")\n",
                "plt.ylabel(\"y\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Conclusion\n",
                "\n",
                "**SGD** takes small steps and struggles to navigate the curved valley. It's slow!\n",
                "\n",
                "**Adam** adapts its learning rate for each parameter:\n",
                "- It builds momentum to go fast in the right direction.\n",
                "- It scales gradients to handle different curvatures.\n",
                "- It reaches the minimum much faster!\n",
                "\n",
                "That's why Adam is the default choice for most deep learning tasks."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}