{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 9. Normalization Methods\n",
                "\n",
                "Training deep networks is hard because the distribution of inputs to each layer changes during training.\n",
                "This is called **Internal Covariate Shift**.\n",
                "Normalization fixes this by forcing layer inputs to have mean 0 and variance 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Batch Normalization (BatchNorm)\n",
                "\n",
                "Normalizes features **across the batch**.\n",
                "\n",
                "For a batch of $N$ samples with $D$ features:\n",
                "1. Calculate mean $\\mu_j$ and variance $\\sigma_j^2$ for each feature $j$ (across $N$ samples).\n",
                "2. Normalize: $\\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}$\n",
                "3. Scale and Shift: $y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j$\n",
                "\n",
                "$\\gamma$ and $\\beta$ are learnable parameters!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CustomBatchNorm1d(nn.Module):\n",
                "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
                "        super().__init__()\n",
                "        self.eps = eps\n",
                "        self.momentum = momentum\n",
                "        \n",
                "        # Learnable parameters\n",
                "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
                "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
                "        \n",
                "        # Running stats (not trained, but updated)\n",
                "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
                "        self.register_buffer('running_var', torch.ones(num_features))\n",
                "\n",
                "    def forward(self, x):\n",
                "        if self.training:\n",
                "            # 1. Calculate batch stats\n",
                "            mean = x.mean(dim=0)\n",
                "            var = x.var(dim=0, unbiased=False)\n",
                "            \n",
                "            # 2. Update running stats (Exponential Moving Average)\n",
                "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
                "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
                "        else:\n",
                "            # Use running stats during inference\n",
                "            mean = self.running_mean\n",
                "            var = self.running_var\n",
                "            \n",
                "        # 3. Normalize\n",
                "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
                "        \n",
                "        # 4. Scale and Shift\n",
                "        out = self.gamma * x_norm + self.beta\n",
                "        return out\n",
                "\n",
                "# Test it\n",
                "batch_size = 3\n",
                "features = 5\n",
                "x = torch.randn(batch_size, features) * 10 + 5 # Mean ~5, Var ~100\n",
                "\n",
                "bn = CustomBatchNorm1d(features)\n",
                "out = bn(x)\n",
                "\n",
                "print(f\"Input Mean: {x.mean(dim=0)}\")\n",
                "print(f\"Output Mean: {out.mean(dim=0)} (Should be close to 0)\")\n",
                "print(f\"Output Var: {out.var(dim=0, unbiased=False)} (Should be close to 1)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Layer Normalization (LayerNorm)\n",
                "\n",
                "Normalizes features **across the sample**.\n",
                "Independent of batch size. Great for RNNs and Transformers.\n",
                "\n",
                "For each sample $i$:\n",
                "1. Calculate mean $\\mu_i$ and variance $\\sigma_i^2$ across all $D$ features.\n",
                "2. Normalize using these sample-specific stats."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ln = nn.LayerNorm(features)\n",
                "out_ln = ln(x)\n",
                "\n",
                "# Check stats per sample (dim=1)\n",
                "print(f\"LayerNorm Output Mean (per sample): {out_ln.mean(dim=1)}\")\n",
                "print(f\"LayerNorm Output Var (per sample): {out_ln.var(dim=1, unbiased=False)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. When to use what?\n",
                "\n",
                "- **BatchNorm**: Default for CNNs and Feed-Forward Networks. Needs decent batch size (>32).\n",
                "- **LayerNorm**: Default for Transformers (BERT, GPT) and RNNs. Works with batch size 1."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}