{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Single Neuron From Scratch\n",
        "\n",
        "A neuron is the building block of neural networks!\n",
        "It takes inputs, multiplies by weights, adds bias, and applies an activation function.\n",
        "Let's build one from scratch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is a Neuron?\n",
        "\n",
        "A neuron takes inputs, multiplies by weights, adds a bias, and applies an activation function!\n",
        "Output = activation(weight1 × input1 + weight2 × input2 + ... + bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single Neuron Calculation:\n",
            "Inputs: tensor([2., 3.])\n",
            "Weights: tensor([0.5000, 0.3000])\n",
            "Bias: 0.1\n",
            "\n",
            "Weighted sum = 0.5 × 2.0 + 0.30000001192092896 × 3.0 + 0.1\n",
            "Weighted sum = 2.00\n",
            "\n",
            "After activation (sigmoid): 0.8808\n",
            "\n",
            "This is how a single neuron works!\n"
          ]
        }
      ],
      "source": [
        "# Manual calculation: single neuron\n",
        "# Inputs\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "\n",
        "# Weights\n",
        "weights = torch.tensor([0.5, 0.3])\n",
        "\n",
        "# Bias\n",
        "bias = 0.1\n",
        "\n",
        "# Compute weighted sum\n",
        "weighted_sum = weights[0] * inputs[0] + weights[1] * inputs[1] + bias\n",
        "\n",
        "print(\"Single Neuron Calculation:\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")\n",
        "print()\n",
        "print(f\"Weighted sum = {weights[0]} × {inputs[0]} + {weights[1]} × {inputs[1]} + {bias}\")\n",
        "print(f\"Weighted sum = {weighted_sum:.2f}\")\n",
        "print()\n",
        "\n",
        "# Apply activation (sigmoid)\n",
        "activation = 1 / (1 + torch.exp(-weighted_sum))\n",
        "print(f\"After activation (sigmoid): {activation:.4f}\")\n",
        "print()\n",
        "print(\"This is how a single neuron works!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Step 1: Initialize a Neuron\n",
        "\n",
        "First, let's create a neuron class that can store weights and bias.\n",
        "We'll build it step by step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Neuron Initialization\n",
            "Number of inputs: 2\n",
            "Weights: tensor([ 0.0166, -0.0366])\n",
            "Bias: 0.0\n",
            "\n",
            "Good! Our neuron now has weights and bias initialized.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create a simple neuron class that just stores weights and bias\n",
        "class Neuron:\n",
        "    \"\"\"Single neuron from scratch - Step 1: Just initialization\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs):\n",
        "        # Initialize weights randomly (small values)\n",
        "        self.weights = torch.randn(num_inputs) * 0.1\n",
        "        # Initialize bias to 0\n",
        "        self.bias = torch.tensor(0.0)\n",
        "\n",
        "# Let's create a neuron and see what it looks like\n",
        "neuron = Neuron(num_inputs=2)\n",
        "\n",
        "print(\"Step 1: Neuron Initialization\")\n",
        "print(f\"Number of inputs: 2\")\n",
        "print(f\"Weights: {neuron.weights}\")\n",
        "print(f\"Bias: {neuron.bias}\")\n",
        "print()\n",
        "print(\"Good! Our neuron now has weights and bias initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Step 2: Calculate Weighted Sum\n",
        "\n",
        "Now let's add a method to calculate the weighted sum (weights · inputs + bias).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Weighted Sum Calculation\n",
            "Inputs: tensor([2., 3.])\n",
            "Weights: tensor([0.5000, 0.3000])\n",
            "Bias: 0.1\n",
            "\n",
            "Weighted sum = weights · inputs + bias\n",
            "              = tensor([0.5000, 0.3000]) · tensor([2., 3.]) + 0.1\n",
            "              = 2.0000\n",
            "\n",
            "Great! Now we can calculate the weighted sum.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Add method to calculate weighted sum\n",
        "class Neuron:\n",
        "    \"\"\"Single neuron from scratch - Step 2: Add weighted sum calculation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs):\n",
        "        # Initialize weights randomly (small values)\n",
        "        self.weights = torch.randn(num_inputs) * 0.1\n",
        "        # Initialize bias to 0\n",
        "        self.bias = torch.tensor(0.0)\n",
        "    \n",
        "    def weighted_sum(self, inputs):\n",
        "        \"\"\"Calculate: weights · inputs + bias\"\"\"\n",
        "        return torch.dot(self.weights, inputs) + self.bias\n",
        "\n",
        "# Test it\n",
        "neuron = Neuron(num_inputs=2)\n",
        "neuron.weights = torch.tensor([0.5, 0.3])  # Set fixed weights for demo\n",
        "neuron.bias = 0.1\n",
        "\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "result = neuron.weighted_sum(inputs)\n",
        "\n",
        "print(\"Step 2: Weighted Sum Calculation\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights: {neuron.weights}\")\n",
        "print(f\"Bias: {neuron.bias}\")\n",
        "print()\n",
        "print(f\"Weighted sum = weights · inputs + bias\")\n",
        "print(f\"              = {neuron.weights} · {inputs} + {neuron.bias}\")\n",
        "print(f\"              = {result:.4f}\")\n",
        "print()\n",
        "print(\"Great! Now we can calculate the weighted sum.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Step 3: Add Activation Function\n",
        "\n",
        "The activation function makes the neuron non-linear. Let's add sigmoid activation first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3: Adding Activation Function\n",
            "1. Weighted sum = 2.0000\n",
            "2. After sigmoid activation = 0.8808\n",
            "\n",
            "Perfect! Now we have a complete neuron with activation.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Add sigmoid activation function\n",
        "class Neuron:\n",
        "    \"\"\"Single neuron from scratch - Step 3: Add sigmoid activation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs):\n",
        "        # Initialize weights randomly (small values)\n",
        "        self.weights = torch.randn(num_inputs) * 0.1\n",
        "        # Initialize bias to 0\n",
        "        self.bias = torch.tensor(0.0)\n",
        "    \n",
        "    def weighted_sum(self, inputs):\n",
        "        \"\"\"Calculate: weights · inputs + bias\"\"\"\n",
        "        return torch.dot(self.weights, inputs) + self.bias\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation: 1 / (1 + exp(-x))\"\"\"\n",
        "        return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Test it step by step\n",
        "neuron = Neuron(num_inputs=2)\n",
        "neuron.weights = torch.tensor([0.5, 0.3])  # Set fixed weights for demo\n",
        "neuron.bias = 0.1\n",
        "\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "\n",
        "# Step 1: Calculate weighted sum\n",
        "ws = neuron.weighted_sum(inputs)\n",
        "print(\"Step 3: Adding Activation Function\")\n",
        "print(f\"1. Weighted sum = {ws:.4f}\")\n",
        "\n",
        "# Step 2: Apply sigmoid\n",
        "output = neuron.sigmoid(ws)\n",
        "print(f\"2. After sigmoid activation = {output:.4f}\")\n",
        "print()\n",
        "print(\"Perfect! Now we have a complete neuron with activation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Step 4: Complete Forward Pass\n",
        "\n",
        "Now let's combine everything into a single `forward` method that does it all!\n",
        "\n",
        "**What a neuron does:**\n",
        "- Takes multiple inputs\n",
        "- Multiplies by weights (importance of each input)\n",
        "- Adds bias (shift the output)\n",
        "- Applies activation function (non-linearity)\n",
        "\n",
        "**Key components:**\n",
        "- **Weights**: How important each input is\n",
        "- **Bias**: Shifts the weighted sum\n",
        "- **Activation**: Makes neuron non-linear (sigmoid, ReLU, tanh, etc.)\n",
        "\n",
        "**Forward pass:**\n",
        "1. Weighted sum = weights · inputs + bias\n",
        "2. Output = activation(weighted sum)\n",
        "\n",
        "**Remember:** A single neuron is the building block of neural networks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4: Complete Forward Pass\n",
            "Inputs: tensor([2., 3.])\n",
            "Weights: tensor([0.5000, 0.3000])\n",
            "Bias: 0.1\n",
            "\n",
            "Output: 0.8808\n",
            "\n",
            "Excellent! Our neuron is complete and working!\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Complete neuron with forward pass\n",
        "class Neuron:\n",
        "    \"\"\"Single neuron from scratch - Complete version\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs):\n",
        "        # Initialize weights randomly (small values)\n",
        "        self.weights = torch.randn(num_inputs) * 0.1\n",
        "        # Initialize bias to 0\n",
        "        self.bias = torch.tensor(0.0)\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation: 1 / (1 + exp(-x))\"\"\"\n",
        "        return 1 / (1 + torch.exp(-x))\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: weighted sum + activation\"\"\"\n",
        "        # Step 1: Calculate weighted sum\n",
        "        weighted_sum = torch.dot(self.weights, inputs) + self.bias\n",
        "        # Step 2: Apply activation\n",
        "        output = self.sigmoid(weighted_sum)\n",
        "        return output\n",
        "\n",
        "# Test the complete neuron\n",
        "neuron = Neuron(num_inputs=2)\n",
        "neuron.weights = torch.tensor([0.5, 0.3])  # Set fixed weights for demo\n",
        "neuron.bias = 0.1\n",
        "\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "output = neuron.forward(inputs)\n",
        "\n",
        "print(\"Step 4: Complete Forward Pass\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights: {neuron.weights}\")\n",
        "print(f\"Bias: {neuron.bias}\")\n",
        "print()\n",
        "print(f\"Output: {output:.4f}\")\n",
        "print()\n",
        "print(\"Excellent! Our neuron is complete and working!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test with Different Inputs\n",
        "\n",
        "Let's see how our neuron responds to different inputs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing neuron with different inputs:\n",
            "Weights: tensor([0.5000, 0.3000]), Bias: 0.1\n",
            "\n",
            "Input          | Weighted Sum | Output (sigmoid)\n",
            "--------------------------------------------------\n",
            "tensor([0., 0.]) |      0.1000 |         0.5250\n",
            "tensor([1., 0.]) |      0.6000 |         0.6457\n",
            "tensor([0., 1.]) |      0.4000 |         0.5987\n",
            "tensor([1., 1.]) |      0.9000 |         0.7109\n",
            "tensor([2., 3.]) |      2.0000 |         0.8808\n"
          ]
        }
      ],
      "source": [
        "# Test neuron with different inputs\n",
        "neuron = Neuron(num_inputs=2)\n",
        "neuron.weights = torch.tensor([0.5, 0.3])\n",
        "neuron.bias = 0.1\n",
        "\n",
        "test_inputs = [\n",
        "    torch.tensor([0.0, 0.0]),\n",
        "    torch.tensor([1.0, 0.0]),\n",
        "    torch.tensor([0.0, 1.0]),\n",
        "    torch.tensor([1.0, 1.0]),\n",
        "    torch.tensor([2.0, 3.0]),\n",
        "]\n",
        "\n",
        "print(\"Testing neuron with different inputs:\")\n",
        "print(f\"Weights: {neuron.weights}, Bias: {neuron.bias}\")\n",
        "print()\n",
        "print(\"Input          | Weighted Sum | Output (sigmoid)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for inp in test_inputs:\n",
        "    weighted_sum = torch.dot(neuron.weights, inp) + neuron.bias\n",
        "    output = neuron.forward(inp)\n",
        "    print(f\"{str(inp):14} | {weighted_sum:11.4f} | {output:14.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Try Different Activation Functions\n",
        "\n",
        "Neurons can use different activation functions! Let's add support for more activations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing different activation functions:\n",
            "Input = 2.0, Weights = [1.0], Bias = 0.0\n",
            "\n",
            "sigmoid : weighted_sum = 2.00 → output = 0.8808\n",
            "relu    : weighted_sum = 2.00 → output = 2.0000\n",
            "tanh    : weighted_sum = 2.00 → output = 0.9640\n",
            "linear  : weighted_sum = 2.00 → output = 2.0000\n"
          ]
        }
      ],
      "source": [
        "# Enhanced neuron with multiple activation functions\n",
        "class Neuron:\n",
        "    \"\"\"Single neuron with multiple activation functions\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, activation='sigmoid'):\n",
        "        # Initialize weights randomly (small values)\n",
        "        self.weights = torch.randn(num_inputs) * 0.1\n",
        "        # Initialize bias to 0\n",
        "        self.bias = torch.tensor(0.0)\n",
        "        self.activation_name = activation\n",
        "    \n",
        "    def activate(self, x):\n",
        "        \"\"\"Apply activation function\"\"\"\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return 1 / (1 + torch.exp(-x))\n",
        "        elif self.activation_name == 'relu':\n",
        "            return torch.maximum(torch.tensor(0.0), x)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            return torch.tanh(x)\n",
        "        else:\n",
        "            return x  # No activation (linear)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: weighted sum + activation\"\"\"\n",
        "        weighted_sum = torch.dot(self.weights, inputs) + self.bias\n",
        "        output = self.activate(weighted_sum)\n",
        "        return output\n",
        "\n",
        "# Test different activation functions\n",
        "print(\"Testing different activation functions:\")\n",
        "print(\"Input = 2.0, Weights = [1.0], Bias = 0.0\")\n",
        "print()\n",
        "\n",
        "activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
        "\n",
        "for act_name in activations:\n",
        "    neuron = Neuron(num_inputs=1, activation=act_name)\n",
        "    neuron.weights = torch.tensor([1.0])\n",
        "    neuron.bias = 0.0\n",
        "    \n",
        "    input_val = torch.tensor([2.0])\n",
        "    weighted_sum = torch.dot(neuron.weights, input_val) + neuron.bias\n",
        "    output = neuron.forward(input_val)\n",
        "    \n",
        "    print(f\"{act_name:8s}: weighted_sum = {weighted_sum:.2f} → output = {output:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Takeaways\n",
        "\n",
        "**What a neuron does:**\n",
        "- Takes multiple inputs\n",
        "- Multiplies by weights (importance of each input)\n",
        "- Adds bias (shift the output)\n",
        "- Applies activation function (non-linearity)\n",
        "\n",
        "**Key components:**\n",
        "- **Weights**: How important each input is\n",
        "- **Bias**: Shifts the weighted sum\n",
        "- **Activation**: Makes neuron non-linear (sigmoid, ReLU, tanh, etc.)\n",
        "\n",
        "**Forward pass:**\n",
        "1. Weighted sum = weights · inputs + bias\n",
        "2. Output = activation(weighted sum)\n",
        "\n",
        "**Remember:** A single neuron is the building block of neural networks!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
