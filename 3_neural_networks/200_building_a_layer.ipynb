{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Building a Layer from Scratch\n",
        "\n",
        "A layer contains multiple neurons!\n",
        "Each neuron in a layer processes the same inputs but with different weights.\n",
        "Let's build a layer from scratch!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is a Layer?\n",
        "\n",
        "A layer is a collection of neurons!\n",
        "All neurons receive the same inputs, but each has its own weights and bias.\n",
        "Output of a layer = vector (one output per neuron).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer with 2 neurons, 2 inputs:\n",
            "Inputs: tensor([2., 3.])\n",
            "Weights shape: torch.Size([2, 2]) (neurons × inputs)\n",
            "Weights:\n",
            "tensor([[0.5000, 0.3000],\n",
            "        [0.2000, 0.4000]])\n",
            "Biases: tensor([0.1000, 0.2000])\n",
            "\n",
            "Forward pass:\n",
            "Outputs = inputs @ weights.T + biases\n",
            "        = tensor([2., 3.]) @ tensor([[0.5000, 0.2000],\n",
            "        [0.3000, 0.4000]]) + tensor([0.1000, 0.2000])\n",
            "        = tensor([2.0000, 1.8000])\n",
            "\n",
            "After activation (sigmoid): tensor([0.8808, 0.8581])\n",
            "\n",
            "Step by step for each neuron:\n",
            "Neuron 1: tensor([0.5000, 0.3000]) · tensor([2., 3.]) + 0.10000000149011612 = 2.0000 → 0.8808\n",
            "Neuron 2: tensor([0.2000, 0.4000]) · tensor([2., 3.]) + 0.20000000298023224 = 1.8000 → 0.8581\n"
          ]
        }
      ],
      "source": [
        "# Manual calculation: layer with 2 neurons, 2 inputs each\n",
        "# Inputs\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "\n",
        "# Layer weights: 2 neurons × 2 inputs = 2×2 matrix\n",
        "# Row i = weights for neuron i\n",
        "weights = torch.tensor([[0.5, 0.3],   # Neuron 1 weights\n",
        "                       [0.2, 0.4]])    # Neuron 2 weights\n",
        "\n",
        "# Biases: one per neuron\n",
        "biases = torch.tensor([0.1, 0.2])\n",
        "\n",
        "print(\"Layer with 2 neurons, 2 inputs:\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights shape: {weights.shape} (neurons × inputs)\")\n",
        "print(f\"Weights:\\n{weights}\")\n",
        "print(f\"Biases: {biases}\")\n",
        "print()\n",
        "\n",
        "# Forward pass: each neuron computes its output\n",
        "# This is just matrix multiplication!\n",
        "outputs = inputs @ weights.T + biases  # weights.T because we need (inputs × weights)\n",
        "\n",
        "print(\"Forward pass:\")\n",
        "print(f\"Outputs = inputs @ weights.T + biases\")\n",
        "print(f\"        = {inputs} @ {weights.T} + {biases}\")\n",
        "print(f\"        = {outputs}\")\n",
        "print()\n",
        "\n",
        "# Apply activation (sigmoid) to each output\n",
        "activation = torch.sigmoid(outputs)\n",
        "print(f\"After activation (sigmoid): {activation}\")\n",
        "\n",
        "# Show step by step for each neuron\n",
        "print(\"\\nStep by step for each neuron:\")\n",
        "for i in range(2):\n",
        "    weighted_sum = torch.dot(weights[i], inputs) + biases[i]\n",
        "    activated = torch.sigmoid(weighted_sum)\n",
        "    print(f\"Neuron {i+1}: {weights[i]} · {inputs} + {biases[i]} = {weighted_sum:.4f} → {activated:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Step 1: Initialize a Layer\n",
        "\n",
        "First, let's create a layer class that can store weights and biases for multiple neurons.\n",
        "We'll build it step by step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Layer Initialization\n",
            "Number of inputs: 2\n",
            "Number of neurons: 3\n",
            "Weights shape: torch.Size([3, 2]) (neurons × inputs)\n",
            "Weights:\n",
            "tensor([[-0.1767, -0.0288],\n",
            "        [ 0.0175, -0.0682],\n",
            "        [-0.0155,  0.0168]])\n",
            "\n",
            "Biases shape: torch.Size([3]) (one per neuron)\n",
            "Biases: tensor([ 0.0342,  0.0569, -0.1017])\n",
            "\n",
            "Good! Our layer now has weights and biases for all neurons initialized.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create a simple layer class that just stores weights and biases\n",
        "class Layer:\n",
        "    \"\"\"Layer with multiple neurons - Step 1: Just initialization\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, num_neurons):\n",
        "        # Weight matrix: (num_neurons, num_inputs)\n",
        "        # Each row is weights for one neuron\n",
        "        self.weights = torch.randn(num_neurons, num_inputs) * 0.1\n",
        "        # Bias vector: one per neuron\n",
        "        self.biases = torch.randn(num_neurons) * 0.1\n",
        "\n",
        "# Let's create a layer and see what it looks like\n",
        "layer = Layer(num_inputs=2, num_neurons=3)\n",
        "\n",
        "print(\"Step 1: Layer Initialization\")\n",
        "print(f\"Number of inputs: 2\")\n",
        "print(f\"Number of neurons: 3\")\n",
        "print(f\"Weights shape: {layer.weights.shape} (neurons × inputs)\")\n",
        "print(f\"Weights:\\n{layer.weights}\")\n",
        "print()\n",
        "print(f\"Biases shape: {layer.biases.shape} (one per neuron)\")\n",
        "print(f\"Biases: {layer.biases}\")\n",
        "print()\n",
        "print(\"Good! Our layer now has weights and biases for all neurons initialized.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Step 2: Calculate Weighted Sums for All Neurons\n",
        "\n",
        "Now let's add a method to calculate weighted sums for all neurons using matrix multiplication!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: Weighted Sum Calculation for All Neurons\n",
            "Inputs: tensor([2., 3.])\n",
            "Weights shape: torch.Size([3, 2])\n",
            "Weights:\n",
            "tensor([[0.5000, 0.3000],\n",
            "        [0.2000, 0.4000],\n",
            "        [0.1000, 0.6000]])\n",
            "Biases: tensor([0.1000, 0.2000, 0.3000])\n",
            "\n",
            "Weighted sums = inputs @ weights.T + biases\n",
            "              = tensor([2., 3.]) @ tensor([[0.5000, 0.2000, 0.1000],\n",
            "        [0.3000, 0.4000, 0.6000]]) + tensor([0.1000, 0.2000, 0.3000])\n",
            "              = tensor([2.0000, 1.8000, 2.3000])\n",
            "\n",
            "Great! Now we can calculate weighted sums for all neurons at once using matrix multiplication!\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Add method to calculate weighted sums for all neurons\n",
        "class Layer:\n",
        "    \"\"\"Layer with multiple neurons - Step 2: Add weighted sum calculation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, num_neurons):\n",
        "        # Weight matrix: (num_neurons, num_inputs)\n",
        "        # Each row is weights for one neuron\n",
        "        self.weights = torch.randn(num_neurons, num_inputs) * 0.1\n",
        "        # Bias vector: one per neuron\n",
        "        self.biases = torch.randn(num_neurons) * 0.1\n",
        "    \n",
        "    def weighted_sum(self, inputs):\n",
        "        \"\"\"Calculate weighted sums for all neurons: inputs @ weights.T + biases\"\"\"\n",
        "        # Matrix multiplication: inputs @ weights.T + biases\n",
        "        # This computes weighted sum for all neurons at once!\n",
        "        return inputs @ self.weights.T + self.biases\n",
        "\n",
        "# Test it\n",
        "layer = Layer(num_inputs=2, num_neurons=3)\n",
        "layer.weights = torch.tensor([[0.5, 0.3],   # Neuron 1 weights\n",
        "                             [0.2, 0.4],    # Neuron 2 weights\n",
        "                             [0.1, 0.6]])   # Neuron 3 weights\n",
        "layer.biases = torch.tensor([0.1, 0.2, 0.3])\n",
        "\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "result = layer.weighted_sum(inputs)\n",
        "\n",
        "print(\"Step 2: Weighted Sum Calculation for All Neurons\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print(f\"Weights shape: {layer.weights.shape}\")\n",
        "print(f\"Weights:\\n{layer.weights}\")\n",
        "print(f\"Biases: {layer.biases}\")\n",
        "print()\n",
        "print(f\"Weighted sums = inputs @ weights.T + biases\")\n",
        "print(f\"              = {inputs} @ {layer.weights.T} + {layer.biases}\")\n",
        "print(f\"              = {result}\")\n",
        "print()\n",
        "print(\"Great! Now we can calculate weighted sums for all neurons at once using matrix multiplication!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Step 3: Add Activation Function\n",
        "\n",
        "The activation function makes neurons non-linear. Let's add sigmoid activation first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3: Adding Activation Function\n",
            "1. Weighted sums = tensor([2.0000, 1.8000, 2.3000])\n",
            "2. After sigmoid activation = tensor([0.8808, 0.8581, 0.9089])\n",
            "\n",
            "Perfect! Now we have a complete layer with activation.\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Add sigmoid activation function\n",
        "class Layer:\n",
        "    \"\"\"Layer with multiple neurons - Step 3: Add sigmoid activation\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, num_neurons):\n",
        "        # Weight matrix: (num_neurons, num_inputs)\n",
        "        # Each row is weights for one neuron\n",
        "        self.weights = torch.randn(num_neurons, num_inputs) * 0.1\n",
        "        # Bias vector: one per neuron\n",
        "        self.biases = torch.randn(num_neurons) * 0.1\n",
        "    \n",
        "    def weighted_sum(self, inputs):\n",
        "        \"\"\"Calculate weighted sums for all neurons: inputs @ weights.T + biases\"\"\"\n",
        "        return inputs @ self.weights.T + self.biases\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation: applied element-wise to all neurons\"\"\"\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Test it step by step\n",
        "layer = Layer(num_inputs=2, num_neurons=3)\n",
        "layer.weights = torch.tensor([[0.5, 0.3],\n",
        "                             [0.2, 0.4],\n",
        "                             [0.1, 0.6]])\n",
        "layer.biases = torch.tensor([0.1, 0.2, 0.3])\n",
        "\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "\n",
        "# Step 1: Calculate weighted sums\n",
        "ws = layer.weighted_sum(inputs)\n",
        "print(\"Step 3: Adding Activation Function\")\n",
        "print(f\"1. Weighted sums = {ws}\")\n",
        "\n",
        "# Step 2: Apply sigmoid to all neurons\n",
        "output = layer.sigmoid(ws)\n",
        "print(f\"2. After sigmoid activation = {output}\")\n",
        "print()\n",
        "print(\"Perfect! Now we have a complete layer with activation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Step 4: Complete Forward Pass\n",
        "\n",
        "Now let's combine everything into a single `forward` method that does it all!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4: Complete Forward Pass\n",
            "Number of inputs: 2\n",
            "Number of neurons: 3\n",
            "Inputs: tensor([2., 3.])\n",
            "\n",
            "Weights shape: torch.Size([3, 2]) (neurons × inputs)\n",
            "Weights:\n",
            "tensor([[0.5000, 0.3000],\n",
            "        [0.2000, 0.4000],\n",
            "        [0.1000, 0.6000]])\n",
            "\n",
            "Biases: tensor([0.1000, 0.2000, 0.3000])\n",
            "\n",
            "Outputs: tensor([0.8808, 0.8581, 0.9089])\n",
            "Output shape: torch.Size([3]) (one output per neuron)\n",
            "\n",
            "Excellent! Our layer is complete and working!\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Complete layer with forward pass\n",
        "class Layer:\n",
        "    \"\"\"Layer with multiple neurons - Complete version\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, num_neurons):\n",
        "        # Weight matrix: (num_neurons, num_inputs)\n",
        "        # Each row is weights for one neuron\n",
        "        self.weights = torch.randn(num_neurons, num_inputs) * 0.1\n",
        "        # Bias vector: one per neuron\n",
        "        self.biases = torch.randn(num_neurons) * 0.1\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation: applied element-wise\"\"\"\n",
        "        return torch.sigmoid(x)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: weighted sums + activation\"\"\"\n",
        "        # Step 1: Calculate weighted sums for all neurons\n",
        "        weighted_sum = inputs @ self.weights.T + self.biases\n",
        "        # Step 2: Apply activation to all neurons\n",
        "        output = self.sigmoid(weighted_sum)\n",
        "        return output\n",
        "\n",
        "# Test the complete layer\n",
        "layer = Layer(num_inputs=2, num_neurons=3)\n",
        "layer.weights = torch.tensor([[0.5, 0.3],\n",
        "                             [0.2, 0.4],\n",
        "                             [0.1, 0.6]])\n",
        "layer.biases = torch.tensor([0.1, 0.2, 0.3])\n",
        "\n",
        "inputs = torch.tensor([2.0, 3.0])\n",
        "outputs = layer.forward(inputs)\n",
        "\n",
        "print(\"Step 4: Complete Forward Pass\")\n",
        "print(f\"Number of inputs: 2\")\n",
        "print(f\"Number of neurons: 3\")\n",
        "print(f\"Inputs: {inputs}\")\n",
        "print()\n",
        "print(f\"Weights shape: {layer.weights.shape} (neurons × inputs)\")\n",
        "print(f\"Weights:\\n{layer.weights}\")\n",
        "print()\n",
        "print(f\"Biases: {layer.biases}\")\n",
        "print()\n",
        "print(f\"Outputs: {outputs}\")\n",
        "print(f\"Output shape: {outputs.shape} (one output per neuron)\")\n",
        "print()\n",
        "print(\"Excellent! Our layer is complete and working!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test with Single Input\n",
        "\n",
        "Let's test our layer with a single input vector!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single input:\n",
            "Input: tensor([1., 2.])\n",
            "Input shape: torch.Size([2])\n",
            "\n",
            "Output: tensor([0.4309, 0.4839, 0.4158])\n",
            "Output shape: torch.Size([3]) (one output per neuron)\n",
            "\n",
            "Notice: One input vector produces one output vector with 3 values (one per neuron)!\n"
          ]
        }
      ],
      "source": [
        "# Test layer with single input\n",
        "layer = Layer(num_inputs=2, num_neurons=3)\n",
        "\n",
        "single_input = torch.tensor([1.0, 2.0])\n",
        "single_output = layer.forward(single_input)\n",
        "\n",
        "print(\"Single input:\")\n",
        "print(f\"Input: {single_input}\")\n",
        "print(f\"Input shape: {single_input.shape}\")\n",
        "print()\n",
        "print(f\"Output: {single_output}\")\n",
        "print(f\"Output shape: {single_output.shape} (one output per neuron)\")\n",
        "print()\n",
        "print(\"Notice: One input vector produces one output vector with 3 values (one per neuron)!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test with Batch of Inputs\n",
        "\n",
        "Layers can process multiple inputs at once! This is called batch processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch of inputs:\n",
            "Input shape: torch.Size([3, 2]) (batch_size=3, num_inputs=2)\n",
            "Inputs:\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.],\n",
            "        [5., 6.]])\n",
            "\n",
            "Output shape: torch.Size([3, 3]) (batch_size=3, num_neurons=3)\n",
            "Outputs:\n",
            "tensor([[0.5357, 0.4388, 0.4490],\n",
            "        [0.5318, 0.3557, 0.4718],\n",
            "        [0.5280, 0.2804, 0.4946]])\n",
            "\n",
            "Notice: Layer processes all inputs in batch simultaneously!\n",
            "Each row of output corresponds to one input in batch.\n",
            "This is much faster than processing one at a time!\n"
          ]
        }
      ],
      "source": [
        "# Test layer with batch of inputs\n",
        "layer = Layer(num_inputs=2, num_neurons=3)\n",
        "\n",
        "# Batch of inputs (3 samples)\n",
        "batch_inputs = torch.tensor([[1.0, 2.0],\n",
        "                             [3.0, 4.0],\n",
        "                             [5.0, 6.0]])\n",
        "\n",
        "batch_outputs = layer.forward(batch_inputs)\n",
        "\n",
        "print(\"Batch of inputs:\")\n",
        "print(f\"Input shape: {batch_inputs.shape} (batch_size=3, num_inputs=2)\")\n",
        "print(f\"Inputs:\\n{batch_inputs}\")\n",
        "print()\n",
        "print(f\"Output shape: {batch_outputs.shape} (batch_size=3, num_neurons=3)\")\n",
        "print(f\"Outputs:\\n{batch_outputs}\")\n",
        "print()\n",
        "print(\"Notice: Layer processes all inputs in batch simultaneously!\")\n",
        "print(\"Each row of output corresponds to one input in batch.\")\n",
        "print(\"This is much faster than processing one at a time!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Try Different Activation Functions\n",
        "\n",
        "Layers can use different activation functions! Let's add support for more activations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing different activation functions:\n",
            "Input = [2.0, 3.0], 2 neurons\n",
            "\n",
            "sigmoid : weighted_sums = tensor([3.5000, 4.0000]) → outputs = tensor([0.9707, 0.9820])\n",
            "relu    : weighted_sums = tensor([3.5000, 4.0000]) → outputs = tensor([3.5000, 4.0000])\n",
            "tanh    : weighted_sums = tensor([3.5000, 4.0000]) → outputs = tensor([0.9982, 0.9993])\n",
            "linear  : weighted_sums = tensor([3.5000, 4.0000]) → outputs = tensor([3.5000, 4.0000])\n"
          ]
        }
      ],
      "source": [
        "# Enhanced layer with multiple activation functions\n",
        "class Layer:\n",
        "    \"\"\"Layer with multiple activation functions\"\"\"\n",
        "    \n",
        "    def __init__(self, num_inputs, num_neurons, activation='sigmoid'):\n",
        "        # Weight matrix: (num_neurons, num_inputs)\n",
        "        # Each row is weights for one neuron\n",
        "        self.weights = torch.randn(num_neurons, num_inputs) * 0.1\n",
        "        # Bias vector: one per neuron\n",
        "        self.biases = torch.randn(num_neurons) * 0.1\n",
        "        self.activation_name = activation\n",
        "    \n",
        "    def activate(self, x):\n",
        "        \"\"\"Apply activation function element-wise\"\"\"\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return torch.sigmoid(x)\n",
        "        elif self.activation_name == 'relu':\n",
        "            return torch.relu(x)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            return torch.tanh(x)\n",
        "        else:\n",
        "            return x  # No activation (linear)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass: weighted sums + activation\"\"\"\n",
        "        weighted_sum = inputs @ self.weights.T + self.biases\n",
        "        output = self.activate(weighted_sum)\n",
        "        return output\n",
        "\n",
        "# Test different activation functions\n",
        "print(\"Testing different activation functions:\")\n",
        "print(\"Input = [2.0, 3.0], 2 neurons\")\n",
        "print()\n",
        "\n",
        "activations = ['sigmoid', 'relu', 'tanh', 'linear']\n",
        "\n",
        "for act_name in activations:\n",
        "    layer = Layer(num_inputs=2, num_neurons=2, activation=act_name)\n",
        "    layer.weights = torch.tensor([[1.0, 0.5],\n",
        "                                 [0.5, 1.0]])\n",
        "    layer.biases = torch.tensor([0.0, 0.0])\n",
        "    \n",
        "    input_val = torch.tensor([2.0, 3.0])\n",
        "    weighted_sum = input_val @ layer.weights.T + layer.biases\n",
        "    output = layer.forward(input_val)\n",
        "    \n",
        "    print(f\"{act_name:8s}: weighted_sums = {weighted_sum} → outputs = {output}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
