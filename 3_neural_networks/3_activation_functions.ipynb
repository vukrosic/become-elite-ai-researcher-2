{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Activation Functions\n",
                "\n",
                "Without activation functions, a neural network is just a big linear regression model.\n",
                "Activation functions introduce **non-linearity**, allowing networks to learn complex patterns.\n",
                "Let's explore the most common ones!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Non-Linearity?\n",
                "\n",
                "If we stack linear layers: $y = W_2(W_1 x) = (W_2 W_1)x = W_{new}x$.\n",
                "It collapses into a single linear layer!\n",
                "\n",
                "We need: $y = W_2(\\sigma(W_1 x))$, where $\\sigma$ is non-linear."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Sigmoid\n",
                "\n",
                "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
                "\n",
                "- **Range**: (0, 1)\n",
                "- **Pros**: Good for probabilities.\n",
                "- **Cons**: \n",
                "    - **Vanishing Gradient**: Gradients are very small for large/small inputs.\n",
                "    - **Not Zero-Centered**: Outputs are always positive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.linspace(-10, 10, 100, requires_grad=True)\n",
                "y = torch.sigmoid(x)\n",
                "y.sum().backward() # Compute gradients\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(x.detach(), y.detach())\n",
                "plt.title(\"Sigmoid Function\")\n",
                "plt.grid(True)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(x.detach(), x.grad)\n",
                "plt.title(\"Sigmoid Derivative\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tanh (Hyperbolic Tangent)\n",
                "\n",
                "$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
                "\n",
                "- **Range**: (-1, 1)\n",
                "- **Pros**: Zero-centered (better for optimization).\n",
                "- **Cons**: Still suffers from vanishing gradients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x.grad.zero_()\n",
                "y = torch.tanh(x)\n",
                "y.sum().backward()\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(x.detach(), y.detach())\n",
                "plt.title(\"Tanh Function\")\n",
                "plt.grid(True)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(x.detach(), x.grad)\n",
                "plt.title(\"Tanh Derivative\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ReLU (Rectified Linear Unit)\n",
                "\n",
                "$ReLU(x) = \\max(0, x)$\n",
                "\n",
                "- **Range**: [0, inf)\n",
                "- **Pros**: \n",
                "    - Computationally efficient.\n",
                "    - No vanishing gradient for positive inputs.\n",
                "- **Cons**: \n",
                "    - **Dying ReLU**: If input < 0, gradient is 0. Neuron can \"die\" and never recover."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x.grad.zero_()\n",
                "y = torch.relu(x)\n",
                "y.sum().backward()\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(x.detach(), y.detach())\n",
                "plt.title(\"ReLU Function\")\n",
                "plt.grid(True)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(x.detach(), x.grad)\n",
                "plt.title(\"ReLU Derivative\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Leaky ReLU\n",
                "\n",
                "$LeakyReLU(x) = \\max(0.01x, x)$\n",
                "\n",
                "- **Pros**: Fixes Dying ReLU by allowing a small gradient for negative inputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x.grad.zero_()\n",
                "y = torch.nn.functional.leaky_relu(x, negative_slope=0.1)\n",
                "y.sum().backward()\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(x.detach(), y.detach())\n",
                "plt.title(\"Leaky ReLU Function\")\n",
                "plt.grid(True)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(x.detach(), x.grad)\n",
                "plt.title(\"Leaky ReLU Derivative\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Conclusion\n",
                "\n",
                "- Use **ReLU** as your default for hidden layers.\n",
                "- Use **Sigmoid** only for the output of binary classification.\n",
                "- Use **Softmax** (next tutorial) for multi-class classification."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}